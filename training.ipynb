{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Import the single DOF finite difference model\n",
    "from mlp_singledof_FD import MLP, MLPProjectionFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajDataset(Dataset):\n",
    "\t\"\"\"Expert Trajectory Dataset.\"\"\"\n",
    "\tdef __init__(self, inp):\n",
    "\t\t\n",
    "\t\t# input\n",
    "\t\tself.inp = inp\n",
    "\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.inp)    \n",
    "\t\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\n",
    "\t\t# Input\n",
    "  \n",
    "\t\tinp = self.inp[idx]\n",
    "\t\t\n",
    "\t\t\t\t \n",
    "\t\treturn torch.tensor(inp).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_uniform_trajectories(key, v_max, num_batch, nvar):\n",
    "    rng = np.random.default_rng(seed=key)\n",
    "    xi_samples = rng.uniform(\n",
    "        low=-v_max,\n",
    "        high=v_max,\n",
    "        size=(num_batch, nvar)\n",
    "    )\n",
    "    return xi_samples, rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for MLP model\n",
    "\n",
    "num_batch = 1000\n",
    "num_dof=1\n",
    "num_steps=50\n",
    "timestep=0.05\n",
    "v_max=1.0\n",
    "a_max=2.0\n",
    "j_max=5.0\n",
    "p_max=180.0*np.pi/180.0 \n",
    "theta_init=0.0\n",
    "nvar_single = num_steps\n",
    "nvar = num_dof * nvar_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15fb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "xi_samples, rng = sample_uniform_trajectories(42, v_max, num_batch, nvar)\n",
    "\n",
    "inp = xi_samples\n",
    "\n",
    "inp_mean, inp_std = inp.mean(), inp.std()\n",
    "\n",
    "print(\"inp\", inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch Dataloader\n",
    "train_dataset = TrajDataset(inp)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=num_batch, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "enc_inp_dim = np.shape(inp)[1] \n",
    "mlp_inp_dim = enc_inp_dim\n",
    "hidden_dim = 1024\n",
    "mlp_out_dim = 2*nvar #( xi_samples- 0:nvar, lamda_smples- nvar:2*nvar)\n",
    "\n",
    "mlp =  MLP(mlp_inp_dim, hidden_dim, mlp_out_dim)\n",
    "\n",
    "\n",
    "\n",
    "model = MLPProjectionFilter(mlp=mlp,num_batch = num_batch,num_dof=num_dof,num_steps=num_steps,\n",
    "\t\t\t\t\t\t\ttimestep=timestep,v_max=v_max,a_max=a_max,j_max=j_max,p_max=p_max, \n",
    "\t\t\t\t\t\t\ttheta_init=theta_init).to(device)\n",
    "\n",
    "print(type(model))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "epochs = 400\n",
    "step, beta = 0, 1.0 # 3.5\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-2, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.1, verbose=True, last_epoch=epochs-1)\n",
    "losses = []\n",
    "last_loss = torch.inf\n",
    "avg_train_loss, avg_primal_loss, avg_fixed_point_loss, avg_projection_loss = [], [], [], []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Train Loop\n",
    "    losses_train, primal_losses, fixed_point_losses, projection_losses = [], [], [], []\n",
    "    \n",
    "    for (inp) in tqdm(train_loader):\n",
    "        \n",
    "        # Input and Output \n",
    "        inp = inp.to(device)\n",
    "        \n",
    "\n",
    "        xi_projected, accumulated_res_fixed_point, accumulated_res_primal, accumulated_res_primal_temp, accumulated_res_fixed_point_temp = model(inp)\n",
    "        primal_loss, fixed_point_loss, projection_loss, loss = model.mlp_loss(accumulated_res_primal, accumulated_res_fixed_point, inp, xi_projected)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad() #clears the gradients of the model parameters\n",
    "        loss.backward() #computes the gradients of the model parameters\n",
    "        #Gradient Norm check\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm\n",
    "                total_norm += param_norm.item() ** 2\n",
    "\n",
    "        total_norm = total_norm ** 0.5\n",
    "        print(f\"Gradient L2 norm: {total_norm:.4f}\")\n",
    "        \n",
    "        ##Gradient Norm clipping\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step() #updates the model parameters\n",
    "        \n",
    "        losses_train.append(loss.detach().cpu().numpy()) \n",
    "        primal_losses.append(primal_loss.detach().cpu().numpy())\n",
    "        fixed_point_losses.append(fixed_point_loss.detach().cpu().numpy())\n",
    "        projection_losses.append(projection_loss.detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:    \n",
    "        print(f\"Epoch: {epoch + 1}, Train Loss: {np.average(losses_train):.4f}, primal loss: {np.average(primal_losses):.4f}, \\\n",
    "    fixed_point loss: {np.average(fixed_point_losses):.4f}, regression loss: {np.average(projection_losses):.4f}\")\n",
    "    #step += 0.07 #0.15\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    os.makedirs(\"./training_weights\", exist_ok=True)\n",
    "    if loss <= last_loss:\n",
    "            torch.save(model.state_dict(), f\"./training_weights/mlp_learned_proj_manipulator.pth\")\n",
    "            last_loss = loss\n",
    "    avg_train_loss.append(np.average(losses_train)), avg_primal_loss.append(np.average(primal_losses)), \\\n",
    "    avg_projection_loss.append(np.average(projection_losses)), avg_fixed_point_loss.append(np.average(fixed_point_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b981e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "#plt.plot(avg_train_loss, label='Total Loss')\n",
    "plt.plot(avg_primal_loss, label='Primal Loss')\n",
    "#plt.plot(avg_fixed_point_loss, label='Fixed-Point Loss')\n",
    "plt.plot(avg_projection_loss, label='Projection Loss')\n",
    "\n",
    "plt.title('Training Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manipulator_torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
