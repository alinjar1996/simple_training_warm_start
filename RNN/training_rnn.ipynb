{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Import the single DOF finite difference model\n",
    "from mlp_singledof_rnn import MLP, MLPProjectionFilter, CustomGRULayer, GRU_Hidden_State, CustomLSTMLayer, LSTM_Hidden_State\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajDataset(Dataset):\n",
    "    \"\"\"Expert Trajectory Dataset.\"\"\"\n",
    "    def __init__(self, inp, theta_init, v_start, v_goal):\n",
    "        # input\n",
    "        self.inp = inp \n",
    "        self.theta_init = theta_init\n",
    "        self.v_start = v_start\n",
    "        self.v_goal = v_goal\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input\n",
    "        inp = self.inp[idx]\n",
    "        theta_init = self.theta_init[idx]\n",
    "        v_start = self.v_start[idx]\n",
    "        v_goal = self.v_goal[idx]\n",
    "\n",
    "        return (torch.tensor(inp).float(),\n",
    "                torch.tensor(theta_init).float(),\n",
    "                torch.tensor(v_start).float(),\n",
    "                torch.tensor(v_goal).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_uniform_trajectories(key, var_min, var_max, dataset_size, nvar):\n",
    "    rng = np.random.default_rng(seed=key)\n",
    "    xi_samples = rng.uniform(\n",
    "        low=var_min,\n",
    "        high=var_max,\n",
    "        size=(dataset_size, nvar)\n",
    "    )\n",
    "    return xi_samples, rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Choose RNN module: LSTM or GRU\")\n",
    "# parser.add_argument(\"--rnn_module\", type=str, default=\"LSTM\", help=\"Choose RNN module: LSTM or GRU\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "#Choose GRU or LSTM here\n",
    "\n",
    "rnn = \"GRU\"\n",
    "#rnn = \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for MLP model\n",
    "\n",
    "num_batch = 1000\n",
    "num_dof=1\n",
    "num_steps=50\n",
    "timestep=0.05\n",
    "v_max=1.0\n",
    "a_max=2.0\n",
    "j_max=5.0\n",
    "p_max=180.0*np.pi/180.0 \n",
    "theta_init=0.0\n",
    "maxiter_projection = 20\n",
    "nvar_single = num_steps\n",
    "nvar = num_dof * nvar_single\n",
    "theta_init_min=0.0\n",
    "theta_init_max=2*np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating number of constraints\n",
    "num_acc = num_steps - 1\n",
    "num_jerk = num_acc - 1\n",
    "num_pos = num_steps\n",
    "num_vel_constraints = 2 * num_steps * num_dof\n",
    "num_acc_constraints = 2 * num_acc * num_dof\n",
    "num_jerk_constraints = 2 * num_jerk * num_dof\n",
    "num_pos_constraints = 2 * num_pos * num_dof\n",
    "num_total_constraints = (num_vel_constraints + num_acc_constraints + \n",
    "                            num_jerk_constraints + num_pos_constraints)\n",
    "\n",
    "##Maximum Iterations\n",
    "maxiter_projection = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15fb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#### creating dataset\n",
    "\n",
    "dataset_size = num_batch*4 #200000\n",
    "\n",
    "theta_init, rng_theta_init = sample_uniform_trajectories(41, var_min= theta_init_min, var_max = theta_init_max, dataset_size=dataset_size, nvar=1)\n",
    "#print(\"theta_init\", theta_init.shape)\n",
    "v_start, rng_v_start = sample_uniform_trajectories(40, var_min =-0.8*v_max, var_max = 0.8*v_max, dataset_size=dataset_size, nvar=1)\n",
    "#print(\"v_start\", v_start.shape)\n",
    "v_goal, rng_v_goal = sample_uniform_trajectories(39, var_min =-0.8*v_max, var_max = 0.8*v_max, dataset_size=dataset_size, nvar=1)\n",
    "\n",
    "#For training\n",
    "xi_samples, rng = sample_uniform_trajectories(42, var_min=-v_max, var_max=v_max ,dataset_size=dataset_size, nvar=nvar)\n",
    "\n",
    "#For validation\n",
    "xi_val, rng_val = sample_uniform_trajectories(43, var_min=-v_max, var_max=v_max ,dataset_size=dataset_size, nvar=nvar)\n",
    "\n",
    "# xi_samples = torch.from_numpy(xi_samples)\n",
    "# xi_val = torch.from_numpy(xi_val)\n",
    "\n",
    "inp = np.hstack(( xi_samples, theta_init, v_start, v_goal))\n",
    "\n",
    "inp_val = np.hstack(( xi_val, theta_init, v_start, v_goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch Dataloader\n",
    "train_dataset = TrajDataset(inp, theta_init, v_start, v_goal)\n",
    "val_dataset = TrajDataset(inp_val, theta_init, v_start, v_goal)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=num_batch, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=num_batch, shuffle=True, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c25ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if rnn == \"GRU\":\n",
    "    print(\"Training with GRU\")\n",
    "    #GRU handling\n",
    "    rnn = \"GRU\"\n",
    "    gru_input_size = 3*num_total_constraints+3*nvar\n",
    "    # print(gru_input_size)\n",
    "    gru_hidden_size = 512\n",
    "    # gru_output_size = (2*nvar)**2+2*nvar\n",
    "    gru_output_size = num_total_constraints+nvar\n",
    "    # gru_context_size = mlp_planner_inp_dim\n",
    "\n",
    "    gru_context = CustomGRULayer(gru_input_size, gru_hidden_size, gru_output_size)\n",
    "\n",
    "    rnn_context = gru_context\n",
    "\n",
    "\n",
    "    input_hidden_state_init = np.shape(inp)[1]\n",
    "    mid_hidden_state_init = 512\n",
    "    out_hidden_state_init = gru_hidden_size\n",
    "\n",
    "    gru_init  =  GRU_Hidden_State(input_hidden_state_init, mid_hidden_state_init, out_hidden_state_init)\n",
    "    \n",
    "    rnn_init = gru_init\n",
    "    ##\n",
    "elif rnn == \"LSTM\":\n",
    "    print(\"Training with LSTM\")\n",
    "    #LSTM handling\n",
    "    rnn = \"LSTM\"\n",
    "    lstm_input_size = 3*num_total_constraints+3*nvar\n",
    "    # print(lstm_input_size)\n",
    "    lstm_hidden_size = 512\n",
    "    # lstm_output_size = (2*nvar)**2+2*nvar\n",
    "    lstm_output_size = num_total_constraints+nvar\n",
    "    # lstm_context_size = mlp_planner_inp_dim\n",
    "\n",
    "    lstm_context = CustomLSTMLayer(lstm_input_size, lstm_hidden_size, lstm_output_size)\n",
    "\n",
    "    rnn_context = lstm_context\n",
    "\n",
    "    input_hidden_state_init = np.shape(inp)[1]\n",
    "    mid_hidden_state_init = 512\n",
    "    out_hidden_state_init = lstm_hidden_size\n",
    "\n",
    "    lstm_init = LSTM_Hidden_State(input_hidden_state_init, mid_hidden_state_init, out_hidden_state_init)\n",
    "\n",
    "    rnn_init = lstm_init\n",
    "\n",
    "    ##\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96510791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc_inp_dim = np.shape(inp)[1] \n",
    "mlp_inp_dim = enc_inp_dim\n",
    "hidden_dim = 1024\n",
    "mlp_out_dim = 2*nvar + num_total_constraints #( xi_samples- 0:nvar, lamda_smples- nvar:2*nvar)\n",
    "\n",
    "mlp =  MLP(mlp_inp_dim, hidden_dim, mlp_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = MLPProjectionFilter(mlp=mlp,rnn_context=rnn_context, rnn_init = rnn_init, num_batch = num_batch,num_dof=num_dof,num_steps=num_steps,\n",
    "\t\t\t\t\t\t\ttimestep=timestep,v_max=v_max,a_max=a_max,j_max=j_max,p_max=p_max, \n",
    "\t\t\t\t\t\t\tmaxiter_projection=maxiter_projection, rnn=rnn).to(device)\n",
    "\n",
    "print(type(model))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "#step, beta = 0, 1.0 # 3.5\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-3, weight_decay=6e-5)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.1, verbose=True)\n",
    "\n",
    "losses = []\n",
    "last_loss = torch.inf\n",
    "avg_train_loss, avg_primal_loss, avg_fixed_point_loss, avg_projection_loss = [], [], [], []\n",
    "avg_val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Train Loop\n",
    "    losses_train, primal_losses, fixed_point_losses, projection_losses = [], [], [], []\n",
    "    \n",
    "    for (inp, theta_init, v_start, v_goal) in tqdm(train_loader):\n",
    "        \n",
    "        # Input and Output \n",
    "        inp = inp.to(device)\n",
    "        theta_init = theta_init.to(device)\n",
    "        v_start = v_start.to(device)\n",
    "        v_goal = v_goal.to(device)\n",
    "        \n",
    "        xi_projected, accumulated_res_fixed_point, accumulated_res_primal, \\\n",
    "        accumulated_res_primal_temp, accumulated_res_fixed_point_temp = model(inp, theta_init, v_start, v_goal, rnn)\n",
    "\n",
    "        xi_samples_inp = inp[:, :nvar]\n",
    "\n",
    "        primal_loss, fixed_point_loss, projection_loss, loss = model.mlp_loss(accumulated_res_primal, \n",
    "                                                                              accumulated_res_fixed_point, xi_samples_inp, xi_projected)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad() #clears the gradients of the model parameters\n",
    "        loss.backward() #computes the gradients of the model parameters\n",
    "        \n",
    "        \n",
    "        # #Gradient Norm check\n",
    "        # total_norm = 0.0\n",
    "        # for p in model.parameters():\n",
    "        #     if p.grad is not None:\n",
    "        #         param_norm = p.grad.data.norm(2)  # L2 norm\n",
    "        #         total_norm += param_norm.item() ** 2\n",
    "\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # print(f\"Gradient L2 norm: {total_norm:.4f}\")\n",
    "        \n",
    "        ##Gradient Norm clipping\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        optimizer.step() #updates the model parameters (e.g. weights and biases)\n",
    "        \n",
    "        losses_train.append(loss.detach().cpu().numpy()) \n",
    "        primal_losses.append(primal_loss.detach().cpu().numpy())\n",
    "        fixed_point_losses.append(fixed_point_loss.detach().cpu().numpy())\n",
    "        projection_losses.append(projection_loss.detach().cpu().numpy())\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (inp_val, theta_init, v_start, v_goal) in tqdm(val_loader):\n",
    "                inp_val = inp_val.to(device)\n",
    "                theta_init = theta_init.to(device)\n",
    "                v_start = v_start.to(device)\n",
    "                v_goal =  v_goal.to(device)\n",
    "\n",
    "                xi_projected, accumulated_res_fixed_point, accumulated_res_primal, \\\n",
    "                accumulated_res_primal_temp, accumulated_res_fixed_point_temp = model(inp_val, theta_init, v_start, v_goal, rnn)\n",
    "\n",
    "                xi_samples_inp_val = inp_val[:, :nvar]\n",
    "\n",
    "                _, _, _, val_loss = model.mlp_loss(\n",
    "                    accumulated_res_primal, accumulated_res_fixed_point, xi_samples_inp_val, xi_projected\n",
    "                )\n",
    "\n",
    "                val_losses.append(val_loss.detach().cpu().numpy())\n",
    "\n",
    "                #print(f\"Validation Loss: {np.average(val_losses):.4f}\")\n",
    "            \n",
    "\n",
    "    if epoch % 2 == 0:    \n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        \n",
    "\n",
    "\n",
    "    #step += 0.07 #0.15\n",
    "    #scheduler.step()\n",
    "\n",
    "    # mean_train_loss = np.mean(losses_train)\n",
    "    # mean_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    os.makedirs(\"./training_weights\", exist_ok=True)\n",
    "    #if mean_val_loss <= last_loss:\n",
    "    if loss <= last_loss:\n",
    "            torch.save(model.state_dict(), f\"./training_weights/mlp_learned_single_dof_{rnn}.pth\")\n",
    "            last_loss = loss\n",
    "\n",
    "    avg_train_loss.append(np.average(losses_train)), avg_primal_loss.append(np.average(primal_losses)), \\\n",
    "    avg_projection_loss.append(np.average(projection_losses)), avg_fixed_point_loss.append(np.average(fixed_point_losses))\n",
    "    avg_val_loss.append(np.average(val_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b981e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "#plt.plot(avg_train_loss, label='Total Loss')\n",
    "plt.plot(avg_primal_loss, label='Primal Loss')\n",
    "plt.plot(avg_fixed_point_loss, label='Fixed-Point Loss')\n",
    "plt.plot(avg_projection_loss, label='Projection Loss')\n",
    "\n",
    "\n",
    "plt.title('Training Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_val_loss, label='Validation Loss')\n",
    "\n",
    "plt.title('Validation Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f784660",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = np.asarray(avg_train_loss)\n",
    "avg_val_loss = np.asarray(avg_val_loss)\n",
    "print(\"Training_loss_end\", avg_train_loss[-1])\n",
    "print(\"Validation_loss_end\", avg_val_loss[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manipulator_torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
